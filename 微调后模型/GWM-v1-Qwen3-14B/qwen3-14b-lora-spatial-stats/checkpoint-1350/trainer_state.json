{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9233270116499592,
  "eval_steps": 500,
  "global_step": 1350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01083717149823896,
      "grad_norm": 3.799668788909912,
      "learning_rate": 6e-05,
      "loss": 8.1695,
      "step": 5
    },
    {
      "epoch": 0.02167434299647792,
      "grad_norm": 5.036798477172852,
      "learning_rate": 0.00014,
      "loss": 6.9802,
      "step": 10
    },
    {
      "epoch": 0.03251151449471688,
      "grad_norm": 16.43882179260254,
      "learning_rate": 0.0001997093023255814,
      "loss": 5.9931,
      "step": 15
    },
    {
      "epoch": 0.04334868599295584,
      "grad_norm": 9.900699615478516,
      "learning_rate": 0.00019898255813953492,
      "loss": 3.4018,
      "step": 20
    },
    {
      "epoch": 0.0541858574911948,
      "grad_norm": 0.298756867647171,
      "learning_rate": 0.00019825581395348837,
      "loss": 1.2326,
      "step": 25
    },
    {
      "epoch": 0.06502302898943375,
      "grad_norm": 0.22154588997364044,
      "learning_rate": 0.00019752906976744188,
      "loss": 0.8255,
      "step": 30
    },
    {
      "epoch": 0.07586020048767272,
      "grad_norm": 0.15900453925132751,
      "learning_rate": 0.00019680232558139536,
      "loss": 0.7936,
      "step": 35
    },
    {
      "epoch": 0.08669737198591168,
      "grad_norm": 0.1659216582775116,
      "learning_rate": 0.00019607558139534884,
      "loss": 0.716,
      "step": 40
    },
    {
      "epoch": 0.09753454348415064,
      "grad_norm": 0.1956975907087326,
      "learning_rate": 0.00019534883720930232,
      "loss": 0.7264,
      "step": 45
    },
    {
      "epoch": 0.1083717149823896,
      "grad_norm": 0.17891734838485718,
      "learning_rate": 0.00019462209302325583,
      "loss": 0.7324,
      "step": 50
    },
    {
      "epoch": 0.11920888648062855,
      "grad_norm": 0.17719584703445435,
      "learning_rate": 0.0001938953488372093,
      "loss": 0.7383,
      "step": 55
    },
    {
      "epoch": 0.1300460579788675,
      "grad_norm": 0.17154870927333832,
      "learning_rate": 0.0001931686046511628,
      "loss": 0.6948,
      "step": 60
    },
    {
      "epoch": 0.14088322947710646,
      "grad_norm": 0.1612040102481842,
      "learning_rate": 0.0001924418604651163,
      "loss": 0.7041,
      "step": 65
    },
    {
      "epoch": 0.15172040097534545,
      "grad_norm": 0.17532214522361755,
      "learning_rate": 0.00019171511627906978,
      "loss": 0.6856,
      "step": 70
    },
    {
      "epoch": 0.1625575724735844,
      "grad_norm": 0.18889252841472626,
      "learning_rate": 0.00019098837209302326,
      "loss": 0.6859,
      "step": 75
    },
    {
      "epoch": 0.17339474397182336,
      "grad_norm": 0.1836552768945694,
      "learning_rate": 0.00019026162790697674,
      "loss": 0.705,
      "step": 80
    },
    {
      "epoch": 0.18423191547006232,
      "grad_norm": 0.16813521087169647,
      "learning_rate": 0.00018953488372093025,
      "loss": 0.6957,
      "step": 85
    },
    {
      "epoch": 0.19506908696830128,
      "grad_norm": 0.1981298327445984,
      "learning_rate": 0.00018880813953488373,
      "loss": 0.7027,
      "step": 90
    },
    {
      "epoch": 0.20590625846654023,
      "grad_norm": 0.2265036404132843,
      "learning_rate": 0.0001880813953488372,
      "loss": 0.6961,
      "step": 95
    },
    {
      "epoch": 0.2167434299647792,
      "grad_norm": 0.31399673223495483,
      "learning_rate": 0.00018735465116279072,
      "loss": 0.6282,
      "step": 100
    },
    {
      "epoch": 0.22758060146301815,
      "grad_norm": 0.2234572321176529,
      "learning_rate": 0.0001866279069767442,
      "loss": 0.7519,
      "step": 105
    },
    {
      "epoch": 0.2384177729612571,
      "grad_norm": 0.1979607492685318,
      "learning_rate": 0.0001859011627906977,
      "loss": 0.67,
      "step": 110
    },
    {
      "epoch": 0.24925494445949606,
      "grad_norm": 0.2100902497768402,
      "learning_rate": 0.00018517441860465116,
      "loss": 0.6981,
      "step": 115
    },
    {
      "epoch": 0.260092115957735,
      "grad_norm": 0.19108527898788452,
      "learning_rate": 0.00018444767441860467,
      "loss": 0.634,
      "step": 120
    },
    {
      "epoch": 0.270929287455974,
      "grad_norm": 0.203053280711174,
      "learning_rate": 0.00018372093023255815,
      "loss": 0.617,
      "step": 125
    },
    {
      "epoch": 0.28176645895421293,
      "grad_norm": 0.21969422698020935,
      "learning_rate": 0.00018299418604651163,
      "loss": 0.6405,
      "step": 130
    },
    {
      "epoch": 0.2926036304524519,
      "grad_norm": 0.21927393972873688,
      "learning_rate": 0.0001822674418604651,
      "loss": 0.6167,
      "step": 135
    },
    {
      "epoch": 0.3034408019506909,
      "grad_norm": 0.23885926604270935,
      "learning_rate": 0.00018154069767441862,
      "loss": 0.7279,
      "step": 140
    },
    {
      "epoch": 0.3142779734489298,
      "grad_norm": 0.2142600566148758,
      "learning_rate": 0.00018081395348837212,
      "loss": 0.666,
      "step": 145
    },
    {
      "epoch": 0.3251151449471688,
      "grad_norm": 0.22195671498775482,
      "learning_rate": 0.00018008720930232558,
      "loss": 0.7123,
      "step": 150
    },
    {
      "epoch": 0.33595231644540774,
      "grad_norm": 0.22596845030784607,
      "learning_rate": 0.00017936046511627909,
      "loss": 0.6869,
      "step": 155
    },
    {
      "epoch": 0.3467894879436467,
      "grad_norm": 0.2073976695537567,
      "learning_rate": 0.00017863372093023257,
      "loss": 0.636,
      "step": 160
    },
    {
      "epoch": 0.35762665944188565,
      "grad_norm": 0.19993242621421814,
      "learning_rate": 0.00017790697674418605,
      "loss": 0.7417,
      "step": 165
    },
    {
      "epoch": 0.36846383094012464,
      "grad_norm": 0.21953997015953064,
      "learning_rate": 0.00017718023255813953,
      "loss": 0.6227,
      "step": 170
    },
    {
      "epoch": 0.37930100243836357,
      "grad_norm": 0.22815002501010895,
      "learning_rate": 0.00017645348837209304,
      "loss": 0.6129,
      "step": 175
    },
    {
      "epoch": 0.39013817393660255,
      "grad_norm": 0.2071816772222519,
      "learning_rate": 0.00017572674418604652,
      "loss": 0.664,
      "step": 180
    },
    {
      "epoch": 0.4009753454348415,
      "grad_norm": 0.20021720230579376,
      "learning_rate": 0.000175,
      "loss": 0.6073,
      "step": 185
    },
    {
      "epoch": 0.41181251693308046,
      "grad_norm": 0.21817010641098022,
      "learning_rate": 0.0001742732558139535,
      "loss": 0.7285,
      "step": 190
    },
    {
      "epoch": 0.42264968843131945,
      "grad_norm": 0.2136356383562088,
      "learning_rate": 0.000173546511627907,
      "loss": 0.6409,
      "step": 195
    },
    {
      "epoch": 0.4334868599295584,
      "grad_norm": 0.23062123358249664,
      "learning_rate": 0.00017281976744186047,
      "loss": 0.6696,
      "step": 200
    },
    {
      "epoch": 0.44432403142779736,
      "grad_norm": 0.26393258571624756,
      "learning_rate": 0.00017209302325581395,
      "loss": 0.6138,
      "step": 205
    },
    {
      "epoch": 0.4551612029260363,
      "grad_norm": 0.23814213275909424,
      "learning_rate": 0.00017136627906976746,
      "loss": 0.6858,
      "step": 210
    },
    {
      "epoch": 0.4659983744242753,
      "grad_norm": 0.22409126162528992,
      "learning_rate": 0.00017063953488372094,
      "loss": 0.6484,
      "step": 215
    },
    {
      "epoch": 0.4768355459225142,
      "grad_norm": 0.2234790176153183,
      "learning_rate": 0.00016991279069767442,
      "loss": 0.6108,
      "step": 220
    },
    {
      "epoch": 0.4876727174207532,
      "grad_norm": 0.23033863306045532,
      "learning_rate": 0.0001691860465116279,
      "loss": 0.6359,
      "step": 225
    },
    {
      "epoch": 0.4985098889189921,
      "grad_norm": 0.23316311836242676,
      "learning_rate": 0.0001684593023255814,
      "loss": 0.6544,
      "step": 230
    },
    {
      "epoch": 0.5093470604172311,
      "grad_norm": 0.2683557868003845,
      "learning_rate": 0.00016773255813953491,
      "loss": 0.6394,
      "step": 235
    },
    {
      "epoch": 0.52018423191547,
      "grad_norm": 0.22846682369709015,
      "learning_rate": 0.00016700581395348837,
      "loss": 0.6864,
      "step": 240
    },
    {
      "epoch": 0.5310214034137091,
      "grad_norm": 0.21557004749774933,
      "learning_rate": 0.00016627906976744188,
      "loss": 0.5993,
      "step": 245
    },
    {
      "epoch": 0.541858574911948,
      "grad_norm": 0.19977660477161407,
      "learning_rate": 0.00016555232558139536,
      "loss": 0.5995,
      "step": 250
    },
    {
      "epoch": 0.5526957464101869,
      "grad_norm": 0.23182478547096252,
      "learning_rate": 0.00016482558139534884,
      "loss": 0.5838,
      "step": 255
    },
    {
      "epoch": 0.5635329179084259,
      "grad_norm": 0.2366848886013031,
      "learning_rate": 0.00016409883720930232,
      "loss": 0.5764,
      "step": 260
    },
    {
      "epoch": 0.5743700894066649,
      "grad_norm": 0.23223242163658142,
      "learning_rate": 0.00016337209302325583,
      "loss": 0.6532,
      "step": 265
    },
    {
      "epoch": 0.5852072609049038,
      "grad_norm": 0.24583548307418823,
      "learning_rate": 0.0001626453488372093,
      "loss": 0.5792,
      "step": 270
    },
    {
      "epoch": 0.5960444324031428,
      "grad_norm": 0.2579714059829712,
      "learning_rate": 0.0001619186046511628,
      "loss": 0.62,
      "step": 275
    },
    {
      "epoch": 0.6068816039013818,
      "grad_norm": 0.2482473999261856,
      "learning_rate": 0.0001611918604651163,
      "loss": 0.6515,
      "step": 280
    },
    {
      "epoch": 0.6177187753996207,
      "grad_norm": 0.25911277532577515,
      "learning_rate": 0.00016046511627906978,
      "loss": 0.6699,
      "step": 285
    },
    {
      "epoch": 0.6285559468978597,
      "grad_norm": 0.2555321455001831,
      "learning_rate": 0.00015973837209302326,
      "loss": 0.6522,
      "step": 290
    },
    {
      "epoch": 0.6393931183960986,
      "grad_norm": 0.24574284255504608,
      "learning_rate": 0.00015901162790697674,
      "loss": 0.665,
      "step": 295
    },
    {
      "epoch": 0.6502302898943376,
      "grad_norm": 0.23811502754688263,
      "learning_rate": 0.00015828488372093025,
      "loss": 0.6006,
      "step": 300
    },
    {
      "epoch": 0.6610674613925765,
      "grad_norm": 0.26788386702537537,
      "learning_rate": 0.00015755813953488373,
      "loss": 0.5671,
      "step": 305
    },
    {
      "epoch": 0.6719046328908155,
      "grad_norm": 0.22838367521762848,
      "learning_rate": 0.0001568313953488372,
      "loss": 0.6105,
      "step": 310
    },
    {
      "epoch": 0.6827418043890544,
      "grad_norm": 0.2172955423593521,
      "learning_rate": 0.00015610465116279072,
      "loss": 0.5324,
      "step": 315
    },
    {
      "epoch": 0.6935789758872934,
      "grad_norm": 0.23733443021774292,
      "learning_rate": 0.0001553779069767442,
      "loss": 0.6864,
      "step": 320
    },
    {
      "epoch": 0.7044161473855324,
      "grad_norm": 0.22708287835121155,
      "learning_rate": 0.00015465116279069768,
      "loss": 0.603,
      "step": 325
    },
    {
      "epoch": 0.7152533188837713,
      "grad_norm": 0.24929779767990112,
      "learning_rate": 0.00015392441860465116,
      "loss": 0.6381,
      "step": 330
    },
    {
      "epoch": 0.7260904903820103,
      "grad_norm": 0.24454452097415924,
      "learning_rate": 0.00015319767441860467,
      "loss": 0.568,
      "step": 335
    },
    {
      "epoch": 0.7369276618802493,
      "grad_norm": 0.251470685005188,
      "learning_rate": 0.00015247093023255815,
      "loss": 0.6558,
      "step": 340
    },
    {
      "epoch": 0.7477648333784882,
      "grad_norm": 0.24428535997867584,
      "learning_rate": 0.00015174418604651163,
      "loss": 0.6003,
      "step": 345
    },
    {
      "epoch": 0.7586020048767271,
      "grad_norm": 0.2701611816883087,
      "learning_rate": 0.0001510174418604651,
      "loss": 0.5755,
      "step": 350
    },
    {
      "epoch": 0.7694391763749662,
      "grad_norm": 0.23427815735340118,
      "learning_rate": 0.00015029069767441862,
      "loss": 0.587,
      "step": 355
    },
    {
      "epoch": 0.7802763478732051,
      "grad_norm": 0.25349876284599304,
      "learning_rate": 0.0001495639534883721,
      "loss": 0.5817,
      "step": 360
    },
    {
      "epoch": 0.791113519371444,
      "grad_norm": 0.21288183331489563,
      "learning_rate": 0.00014883720930232558,
      "loss": 0.5263,
      "step": 365
    },
    {
      "epoch": 0.801950690869683,
      "grad_norm": 0.24520711600780487,
      "learning_rate": 0.00014811046511627909,
      "loss": 0.5917,
      "step": 370
    },
    {
      "epoch": 0.812787862367922,
      "grad_norm": 0.2550179660320282,
      "learning_rate": 0.00014738372093023257,
      "loss": 0.5896,
      "step": 375
    },
    {
      "epoch": 0.8236250338661609,
      "grad_norm": 0.22628551721572876,
      "learning_rate": 0.00014665697674418605,
      "loss": 0.5498,
      "step": 380
    },
    {
      "epoch": 0.8344622053643999,
      "grad_norm": 0.2870338559150696,
      "learning_rate": 0.00014593023255813953,
      "loss": 0.6056,
      "step": 385
    },
    {
      "epoch": 0.8452993768626389,
      "grad_norm": 0.2725379765033722,
      "learning_rate": 0.00014520348837209304,
      "loss": 0.6246,
      "step": 390
    },
    {
      "epoch": 0.8561365483608778,
      "grad_norm": 0.28505048155784607,
      "learning_rate": 0.00014447674418604652,
      "loss": 0.5869,
      "step": 395
    },
    {
      "epoch": 0.8669737198591168,
      "grad_norm": 0.2284258008003235,
      "learning_rate": 0.00014375,
      "loss": 0.5636,
      "step": 400
    },
    {
      "epoch": 0.8778108913573557,
      "grad_norm": 0.25309839844703674,
      "learning_rate": 0.0001430232558139535,
      "loss": 0.6194,
      "step": 405
    },
    {
      "epoch": 0.8886480628555947,
      "grad_norm": 0.25458112359046936,
      "learning_rate": 0.00014229651162790699,
      "loss": 0.6261,
      "step": 410
    },
    {
      "epoch": 0.8994852343538337,
      "grad_norm": 0.283562570810318,
      "learning_rate": 0.00014156976744186047,
      "loss": 0.6239,
      "step": 415
    },
    {
      "epoch": 0.9103224058520726,
      "grad_norm": 0.2630622684955597,
      "learning_rate": 0.00014084302325581395,
      "loss": 0.5744,
      "step": 420
    },
    {
      "epoch": 0.9211595773503116,
      "grad_norm": 0.26670706272125244,
      "learning_rate": 0.00014011627906976746,
      "loss": 0.5759,
      "step": 425
    },
    {
      "epoch": 0.9319967488485505,
      "grad_norm": 0.293194442987442,
      "learning_rate": 0.00013938953488372094,
      "loss": 0.5828,
      "step": 430
    },
    {
      "epoch": 0.9428339203467895,
      "grad_norm": 0.2591293156147003,
      "learning_rate": 0.00013866279069767442,
      "loss": 0.5803,
      "step": 435
    },
    {
      "epoch": 0.9536710918450284,
      "grad_norm": 0.2565516233444214,
      "learning_rate": 0.0001379360465116279,
      "loss": 0.6202,
      "step": 440
    },
    {
      "epoch": 0.9645082633432674,
      "grad_norm": 0.2594803273677826,
      "learning_rate": 0.0001372093023255814,
      "loss": 0.5859,
      "step": 445
    },
    {
      "epoch": 0.9753454348415064,
      "grad_norm": 0.26312029361724854,
      "learning_rate": 0.0001364825581395349,
      "loss": 0.6375,
      "step": 450
    },
    {
      "epoch": 0.9861826063397453,
      "grad_norm": 0.23501034080982208,
      "learning_rate": 0.00013575581395348837,
      "loss": 0.5242,
      "step": 455
    },
    {
      "epoch": 0.9970197778379842,
      "grad_norm": 0.2885182797908783,
      "learning_rate": 0.00013502906976744188,
      "loss": 0.5798,
      "step": 460
    },
    {
      "epoch": 1.0065023028989433,
      "grad_norm": 0.24063123762607574,
      "learning_rate": 0.00013430232558139536,
      "loss": 0.5422,
      "step": 465
    },
    {
      "epoch": 1.0173394743971824,
      "grad_norm": 0.29459142684936523,
      "learning_rate": 0.00013357558139534884,
      "loss": 0.6293,
      "step": 470
    },
    {
      "epoch": 1.0281766458954213,
      "grad_norm": 0.2945607900619507,
      "learning_rate": 0.00013284883720930232,
      "loss": 0.5365,
      "step": 475
    },
    {
      "epoch": 1.0390138173936603,
      "grad_norm": 0.31159666180610657,
      "learning_rate": 0.00013212209302325583,
      "loss": 0.5782,
      "step": 480
    },
    {
      "epoch": 1.0498509888918992,
      "grad_norm": 0.28740882873535156,
      "learning_rate": 0.0001313953488372093,
      "loss": 0.591,
      "step": 485
    },
    {
      "epoch": 1.0606881603901381,
      "grad_norm": 0.3200608491897583,
      "learning_rate": 0.0001306686046511628,
      "loss": 0.5722,
      "step": 490
    },
    {
      "epoch": 1.071525331888377,
      "grad_norm": 0.27238577604293823,
      "learning_rate": 0.0001299418604651163,
      "loss": 0.6081,
      "step": 495
    },
    {
      "epoch": 1.082362503386616,
      "grad_norm": 0.2543793320655823,
      "learning_rate": 0.00012921511627906978,
      "loss": 0.5786,
      "step": 500
    },
    {
      "epoch": 1.0931996748848551,
      "grad_norm": 0.26465779542922974,
      "learning_rate": 0.00012848837209302326,
      "loss": 0.6114,
      "step": 505
    },
    {
      "epoch": 1.104036846383094,
      "grad_norm": 0.3102627396583557,
      "learning_rate": 0.00012776162790697674,
      "loss": 0.609,
      "step": 510
    },
    {
      "epoch": 1.114874017881333,
      "grad_norm": 0.27102622389793396,
      "learning_rate": 0.00012703488372093025,
      "loss": 0.6187,
      "step": 515
    },
    {
      "epoch": 1.125711189379572,
      "grad_norm": 0.29167473316192627,
      "learning_rate": 0.00012630813953488373,
      "loss": 0.6172,
      "step": 520
    },
    {
      "epoch": 1.1365483608778109,
      "grad_norm": 0.29427528381347656,
      "learning_rate": 0.0001255813953488372,
      "loss": 0.5775,
      "step": 525
    },
    {
      "epoch": 1.1473855323760498,
      "grad_norm": 0.2836896479129791,
      "learning_rate": 0.00012485465116279072,
      "loss": 0.5046,
      "step": 530
    },
    {
      "epoch": 1.1582227038742887,
      "grad_norm": 0.2728812098503113,
      "learning_rate": 0.0001241279069767442,
      "loss": 0.5576,
      "step": 535
    },
    {
      "epoch": 1.1690598753725276,
      "grad_norm": 0.3089021146297455,
      "learning_rate": 0.00012340116279069768,
      "loss": 0.6007,
      "step": 540
    },
    {
      "epoch": 1.1798970468707668,
      "grad_norm": 0.3003908097743988,
      "learning_rate": 0.00012267441860465116,
      "loss": 0.5643,
      "step": 545
    },
    {
      "epoch": 1.1907342183690057,
      "grad_norm": 0.2801857888698578,
      "learning_rate": 0.00012194767441860467,
      "loss": 0.5546,
      "step": 550
    },
    {
      "epoch": 1.2015713898672447,
      "grad_norm": 0.31009310483932495,
      "learning_rate": 0.00012122093023255813,
      "loss": 0.5484,
      "step": 555
    },
    {
      "epoch": 1.2124085613654836,
      "grad_norm": 0.3275088369846344,
      "learning_rate": 0.00012049418604651164,
      "loss": 0.6283,
      "step": 560
    },
    {
      "epoch": 1.2232457328637225,
      "grad_norm": 0.2630678415298462,
      "learning_rate": 0.00011976744186046511,
      "loss": 0.623,
      "step": 565
    },
    {
      "epoch": 1.2340829043619614,
      "grad_norm": 0.28126147389411926,
      "learning_rate": 0.00011904069767441862,
      "loss": 0.549,
      "step": 570
    },
    {
      "epoch": 1.2449200758602004,
      "grad_norm": 0.21871179342269897,
      "learning_rate": 0.00011831395348837211,
      "loss": 0.5383,
      "step": 575
    },
    {
      "epoch": 1.2557572473584395,
      "grad_norm": 0.26850220561027527,
      "learning_rate": 0.00011758720930232559,
      "loss": 0.5449,
      "step": 580
    },
    {
      "epoch": 1.2665944188566785,
      "grad_norm": 0.2893202304840088,
      "learning_rate": 0.00011686046511627909,
      "loss": 0.5828,
      "step": 585
    },
    {
      "epoch": 1.2774315903549174,
      "grad_norm": 0.3013814091682434,
      "learning_rate": 0.00011613372093023255,
      "loss": 0.5927,
      "step": 590
    },
    {
      "epoch": 1.2882687618531563,
      "grad_norm": 0.24172918498516083,
      "learning_rate": 0.00011540697674418606,
      "loss": 0.6146,
      "step": 595
    },
    {
      "epoch": 1.2991059333513952,
      "grad_norm": 0.2922045886516571,
      "learning_rate": 0.00011468023255813953,
      "loss": 0.5946,
      "step": 600
    },
    {
      "epoch": 1.3099431048496342,
      "grad_norm": 0.28440383076667786,
      "learning_rate": 0.00011395348837209304,
      "loss": 0.6088,
      "step": 605
    },
    {
      "epoch": 1.320780276347873,
      "grad_norm": 0.3176741898059845,
      "learning_rate": 0.0001132267441860465,
      "loss": 0.6281,
      "step": 610
    },
    {
      "epoch": 1.3316174478461122,
      "grad_norm": 0.28103911876678467,
      "learning_rate": 0.00011250000000000001,
      "loss": 0.5421,
      "step": 615
    },
    {
      "epoch": 1.3424546193443512,
      "grad_norm": 0.2916892468929291,
      "learning_rate": 0.0001117732558139535,
      "loss": 0.601,
      "step": 620
    },
    {
      "epoch": 1.35329179084259,
      "grad_norm": 0.29176443815231323,
      "learning_rate": 0.00011104651162790699,
      "loss": 0.5536,
      "step": 625
    },
    {
      "epoch": 1.364128962340829,
      "grad_norm": 0.33199435472488403,
      "learning_rate": 0.00011031976744186048,
      "loss": 0.5679,
      "step": 630
    },
    {
      "epoch": 1.374966133839068,
      "grad_norm": 0.3005814552307129,
      "learning_rate": 0.00010959302325581395,
      "loss": 0.5791,
      "step": 635
    },
    {
      "epoch": 1.385803305337307,
      "grad_norm": 0.30415651202201843,
      "learning_rate": 0.00010886627906976746,
      "loss": 0.6026,
      "step": 640
    },
    {
      "epoch": 1.3966404768355458,
      "grad_norm": 0.2879249155521393,
      "learning_rate": 0.00010813953488372092,
      "loss": 0.5371,
      "step": 645
    },
    {
      "epoch": 1.407477648333785,
      "grad_norm": 0.29780712723731995,
      "learning_rate": 0.00010741279069767443,
      "loss": 0.6023,
      "step": 650
    },
    {
      "epoch": 1.418314819832024,
      "grad_norm": 0.2780402600765228,
      "learning_rate": 0.0001066860465116279,
      "loss": 0.5848,
      "step": 655
    },
    {
      "epoch": 1.4291519913302628,
      "grad_norm": 0.3332846462726593,
      "learning_rate": 0.0001059593023255814,
      "loss": 0.6074,
      "step": 660
    },
    {
      "epoch": 1.4399891628285018,
      "grad_norm": 0.33569130301475525,
      "learning_rate": 0.0001052325581395349,
      "loss": 0.6706,
      "step": 665
    },
    {
      "epoch": 1.4508263343267407,
      "grad_norm": 0.27113598585128784,
      "learning_rate": 0.00010450581395348838,
      "loss": 0.5932,
      "step": 670
    },
    {
      "epoch": 1.4616635058249796,
      "grad_norm": 0.2792212665081024,
      "learning_rate": 0.00010377906976744187,
      "loss": 0.5161,
      "step": 675
    },
    {
      "epoch": 1.4725006773232185,
      "grad_norm": 0.30875492095947266,
      "learning_rate": 0.00010305232558139534,
      "loss": 0.6423,
      "step": 680
    },
    {
      "epoch": 1.4833378488214577,
      "grad_norm": 0.2575356364250183,
      "learning_rate": 0.00010232558139534885,
      "loss": 0.6177,
      "step": 685
    },
    {
      "epoch": 1.4941750203196966,
      "grad_norm": 0.3103737235069275,
      "learning_rate": 0.00010159883720930232,
      "loss": 0.5633,
      "step": 690
    },
    {
      "epoch": 1.5050121918179356,
      "grad_norm": 0.31784912943840027,
      "learning_rate": 0.00010087209302325583,
      "loss": 0.5811,
      "step": 695
    },
    {
      "epoch": 1.5158493633161745,
      "grad_norm": 0.3004825711250305,
      "learning_rate": 0.00010014534883720929,
      "loss": 0.5415,
      "step": 700
    },
    {
      "epoch": 1.5266865348144134,
      "grad_norm": 0.28010669350624084,
      "learning_rate": 9.94186046511628e-05,
      "loss": 0.5782,
      "step": 705
    },
    {
      "epoch": 1.5375237063126526,
      "grad_norm": 0.2792551517486572,
      "learning_rate": 9.869186046511628e-05,
      "loss": 0.5894,
      "step": 710
    },
    {
      "epoch": 1.5483608778108913,
      "grad_norm": 0.3039059638977051,
      "learning_rate": 9.796511627906976e-05,
      "loss": 0.5805,
      "step": 715
    },
    {
      "epoch": 1.5591980493091304,
      "grad_norm": 0.29786252975463867,
      "learning_rate": 9.723837209302326e-05,
      "loss": 0.5717,
      "step": 720
    },
    {
      "epoch": 1.5700352208073691,
      "grad_norm": 0.28751155734062195,
      "learning_rate": 9.651162790697675e-05,
      "loss": 0.5366,
      "step": 725
    },
    {
      "epoch": 1.5808723923056083,
      "grad_norm": 0.35797128081321716,
      "learning_rate": 9.578488372093024e-05,
      "loss": 0.6427,
      "step": 730
    },
    {
      "epoch": 1.5917095638038472,
      "grad_norm": 0.35083627700805664,
      "learning_rate": 9.505813953488373e-05,
      "loss": 0.5373,
      "step": 735
    },
    {
      "epoch": 1.6025467353020861,
      "grad_norm": 0.2733335494995117,
      "learning_rate": 9.433139534883722e-05,
      "loss": 0.5676,
      "step": 740
    },
    {
      "epoch": 1.613383906800325,
      "grad_norm": 0.3153267204761505,
      "learning_rate": 9.36046511627907e-05,
      "loss": 0.6257,
      "step": 745
    },
    {
      "epoch": 1.624221078298564,
      "grad_norm": 0.30124735832214355,
      "learning_rate": 9.28779069767442e-05,
      "loss": 0.5856,
      "step": 750
    },
    {
      "epoch": 1.6350582497968031,
      "grad_norm": 0.3692014813423157,
      "learning_rate": 9.215116279069768e-05,
      "loss": 0.5662,
      "step": 755
    },
    {
      "epoch": 1.6458954212950418,
      "grad_norm": 0.3124293088912964,
      "learning_rate": 9.142441860465116e-05,
      "loss": 0.5922,
      "step": 760
    },
    {
      "epoch": 1.656732592793281,
      "grad_norm": 0.29784485697746277,
      "learning_rate": 9.069767441860465e-05,
      "loss": 0.578,
      "step": 765
    },
    {
      "epoch": 1.66756976429152,
      "grad_norm": 0.2865789532661438,
      "learning_rate": 8.997093023255815e-05,
      "loss": 0.5278,
      "step": 770
    },
    {
      "epoch": 1.6784069357897589,
      "grad_norm": 0.33860182762145996,
      "learning_rate": 8.924418604651164e-05,
      "loss": 0.5881,
      "step": 775
    },
    {
      "epoch": 1.6892441072879978,
      "grad_norm": 0.3821052312850952,
      "learning_rate": 8.851744186046512e-05,
      "loss": 0.5926,
      "step": 780
    },
    {
      "epoch": 1.7000812787862367,
      "grad_norm": 0.3489053249359131,
      "learning_rate": 8.779069767441861e-05,
      "loss": 0.5759,
      "step": 785
    },
    {
      "epoch": 1.7109184502844759,
      "grad_norm": 0.3152927756309509,
      "learning_rate": 8.70639534883721e-05,
      "loss": 0.6404,
      "step": 790
    },
    {
      "epoch": 1.7217556217827146,
      "grad_norm": 0.3234851360321045,
      "learning_rate": 8.633720930232559e-05,
      "loss": 0.6084,
      "step": 795
    },
    {
      "epoch": 1.7325927932809537,
      "grad_norm": 0.3192118704319,
      "learning_rate": 8.561046511627907e-05,
      "loss": 0.6091,
      "step": 800
    },
    {
      "epoch": 1.7434299647791927,
      "grad_norm": 0.3064664602279663,
      "learning_rate": 8.488372093023255e-05,
      "loss": 0.636,
      "step": 805
    },
    {
      "epoch": 1.7542671362774316,
      "grad_norm": 0.2892475128173828,
      "learning_rate": 8.415697674418606e-05,
      "loss": 0.5807,
      "step": 810
    },
    {
      "epoch": 1.7651043077756705,
      "grad_norm": 0.33224207162857056,
      "learning_rate": 8.343023255813954e-05,
      "loss": 0.6569,
      "step": 815
    },
    {
      "epoch": 1.7759414792739094,
      "grad_norm": 0.27537330985069275,
      "learning_rate": 8.270348837209303e-05,
      "loss": 0.5538,
      "step": 820
    },
    {
      "epoch": 1.7867786507721486,
      "grad_norm": 0.2464342564344406,
      "learning_rate": 8.197674418604652e-05,
      "loss": 0.5404,
      "step": 825
    },
    {
      "epoch": 1.7976158222703873,
      "grad_norm": 0.33874455094337463,
      "learning_rate": 8.125000000000001e-05,
      "loss": 0.6372,
      "step": 830
    },
    {
      "epoch": 1.8084529937686264,
      "grad_norm": 0.3706772029399872,
      "learning_rate": 8.052325581395349e-05,
      "loss": 0.5467,
      "step": 835
    },
    {
      "epoch": 1.8192901652668654,
      "grad_norm": 0.3379359245300293,
      "learning_rate": 7.979651162790697e-05,
      "loss": 0.5364,
      "step": 840
    },
    {
      "epoch": 1.8301273367651043,
      "grad_norm": 0.2915709614753723,
      "learning_rate": 7.906976744186047e-05,
      "loss": 0.5912,
      "step": 845
    },
    {
      "epoch": 1.8409645082633432,
      "grad_norm": 0.2961192727088928,
      "learning_rate": 7.834302325581395e-05,
      "loss": 0.5361,
      "step": 850
    },
    {
      "epoch": 1.8518016797615822,
      "grad_norm": 0.2693919241428375,
      "learning_rate": 7.761627906976745e-05,
      "loss": 0.4868,
      "step": 855
    },
    {
      "epoch": 1.8626388512598213,
      "grad_norm": 0.2512694001197815,
      "learning_rate": 7.688953488372094e-05,
      "loss": 0.5349,
      "step": 860
    },
    {
      "epoch": 1.87347602275806,
      "grad_norm": 0.3135937452316284,
      "learning_rate": 7.616279069767443e-05,
      "loss": 0.5876,
      "step": 865
    },
    {
      "epoch": 1.8843131942562992,
      "grad_norm": 0.31200942397117615,
      "learning_rate": 7.543604651162791e-05,
      "loss": 0.5388,
      "step": 870
    },
    {
      "epoch": 1.895150365754538,
      "grad_norm": 0.34462711215019226,
      "learning_rate": 7.47093023255814e-05,
      "loss": 0.6056,
      "step": 875
    },
    {
      "epoch": 1.905987537252777,
      "grad_norm": 0.34289124608039856,
      "learning_rate": 7.398255813953489e-05,
      "loss": 0.5909,
      "step": 880
    },
    {
      "epoch": 1.916824708751016,
      "grad_norm": 0.2990979850292206,
      "learning_rate": 7.325581395348837e-05,
      "loss": 0.5648,
      "step": 885
    },
    {
      "epoch": 1.9276618802492549,
      "grad_norm": 0.3129749000072479,
      "learning_rate": 7.252906976744186e-05,
      "loss": 0.6024,
      "step": 890
    },
    {
      "epoch": 1.938499051747494,
      "grad_norm": 0.28061485290527344,
      "learning_rate": 7.180232558139535e-05,
      "loss": 0.5278,
      "step": 895
    },
    {
      "epoch": 1.9493362232457327,
      "grad_norm": 0.28440701961517334,
      "learning_rate": 7.107558139534885e-05,
      "loss": 0.5859,
      "step": 900
    },
    {
      "epoch": 1.960173394743972,
      "grad_norm": 0.3109729588031769,
      "learning_rate": 7.034883720930233e-05,
      "loss": 0.571,
      "step": 905
    },
    {
      "epoch": 1.9710105662422108,
      "grad_norm": 0.32991117238998413,
      "learning_rate": 6.962209302325582e-05,
      "loss": 0.6502,
      "step": 910
    },
    {
      "epoch": 1.9818477377404498,
      "grad_norm": 0.26307931542396545,
      "learning_rate": 6.88953488372093e-05,
      "loss": 0.5435,
      "step": 915
    },
    {
      "epoch": 1.9926849092386887,
      "grad_norm": 0.29558539390563965,
      "learning_rate": 6.81686046511628e-05,
      "loss": 0.5892,
      "step": 920
    },
    {
      "epoch": 2.002167434299648,
      "grad_norm": 0.2747756540775299,
      "learning_rate": 6.744186046511628e-05,
      "loss": 0.5995,
      "step": 925
    },
    {
      "epoch": 2.0130046057978865,
      "grad_norm": 0.33122503757476807,
      "learning_rate": 6.671511627906976e-05,
      "loss": 0.5955,
      "step": 930
    },
    {
      "epoch": 2.0238417772961257,
      "grad_norm": 0.3113971948623657,
      "learning_rate": 6.598837209302326e-05,
      "loss": 0.5514,
      "step": 935
    },
    {
      "epoch": 2.034678948794365,
      "grad_norm": 0.29420343041419983,
      "learning_rate": 6.526162790697675e-05,
      "loss": 0.567,
      "step": 940
    },
    {
      "epoch": 2.0455161202926035,
      "grad_norm": 0.2725427746772766,
      "learning_rate": 6.453488372093024e-05,
      "loss": 0.5203,
      "step": 945
    },
    {
      "epoch": 2.0563532917908427,
      "grad_norm": 0.30203908681869507,
      "learning_rate": 6.380813953488373e-05,
      "loss": 0.5454,
      "step": 950
    },
    {
      "epoch": 2.0671904632890814,
      "grad_norm": 0.3708071708679199,
      "learning_rate": 6.308139534883722e-05,
      "loss": 0.5658,
      "step": 955
    },
    {
      "epoch": 2.0780276347873206,
      "grad_norm": 0.31567299365997314,
      "learning_rate": 6.23546511627907e-05,
      "loss": 0.5891,
      "step": 960
    },
    {
      "epoch": 2.0888648062855593,
      "grad_norm": 0.29505038261413574,
      "learning_rate": 6.162790697674418e-05,
      "loss": 0.5867,
      "step": 965
    },
    {
      "epoch": 2.0997019777837984,
      "grad_norm": 0.3134653568267822,
      "learning_rate": 6.0901162790697675e-05,
      "loss": 0.5165,
      "step": 970
    },
    {
      "epoch": 2.1105391492820376,
      "grad_norm": 0.27133461833000183,
      "learning_rate": 6.017441860465116e-05,
      "loss": 0.5006,
      "step": 975
    },
    {
      "epoch": 2.1213763207802763,
      "grad_norm": 0.3023398220539093,
      "learning_rate": 5.944767441860465e-05,
      "loss": 0.5605,
      "step": 980
    },
    {
      "epoch": 2.1322134922785154,
      "grad_norm": 0.3190254867076874,
      "learning_rate": 5.8720930232558145e-05,
      "loss": 0.5914,
      "step": 985
    },
    {
      "epoch": 2.143050663776754,
      "grad_norm": 0.32254454493522644,
      "learning_rate": 5.799418604651163e-05,
      "loss": 0.5736,
      "step": 990
    },
    {
      "epoch": 2.1538878352749933,
      "grad_norm": 0.31620359420776367,
      "learning_rate": 5.726744186046512e-05,
      "loss": 0.6123,
      "step": 995
    },
    {
      "epoch": 2.164725006773232,
      "grad_norm": 0.3158670961856842,
      "learning_rate": 5.654069767441861e-05,
      "loss": 0.5134,
      "step": 1000
    },
    {
      "epoch": 2.175562178271471,
      "grad_norm": 0.33916351199150085,
      "learning_rate": 5.5813953488372095e-05,
      "loss": 0.5513,
      "step": 1005
    },
    {
      "epoch": 2.1863993497697103,
      "grad_norm": 0.38089844584465027,
      "learning_rate": 5.508720930232558e-05,
      "loss": 0.5416,
      "step": 1010
    },
    {
      "epoch": 2.197236521267949,
      "grad_norm": 0.30705249309539795,
      "learning_rate": 5.436046511627907e-05,
      "loss": 0.5718,
      "step": 1015
    },
    {
      "epoch": 2.208073692766188,
      "grad_norm": 0.3505074977874756,
      "learning_rate": 5.363372093023256e-05,
      "loss": 0.5537,
      "step": 1020
    },
    {
      "epoch": 2.218910864264427,
      "grad_norm": 0.3462381064891815,
      "learning_rate": 5.290697674418605e-05,
      "loss": 0.5924,
      "step": 1025
    },
    {
      "epoch": 2.229748035762666,
      "grad_norm": 0.30699416995048523,
      "learning_rate": 5.218023255813954e-05,
      "loss": 0.5707,
      "step": 1030
    },
    {
      "epoch": 2.2405852072609047,
      "grad_norm": 0.34770792722702026,
      "learning_rate": 5.145348837209303e-05,
      "loss": 0.5228,
      "step": 1035
    },
    {
      "epoch": 2.251422378759144,
      "grad_norm": 0.32112956047058105,
      "learning_rate": 5.0726744186046515e-05,
      "loss": 0.5591,
      "step": 1040
    },
    {
      "epoch": 2.262259550257383,
      "grad_norm": 0.33205100893974304,
      "learning_rate": 5e-05,
      "loss": 0.6144,
      "step": 1045
    },
    {
      "epoch": 2.2730967217556217,
      "grad_norm": 0.37360861897468567,
      "learning_rate": 4.927325581395349e-05,
      "loss": 0.5996,
      "step": 1050
    },
    {
      "epoch": 2.283933893253861,
      "grad_norm": 0.3363434970378876,
      "learning_rate": 4.854651162790698e-05,
      "loss": 0.5461,
      "step": 1055
    },
    {
      "epoch": 2.2947710647520996,
      "grad_norm": 0.3064538538455963,
      "learning_rate": 4.781976744186047e-05,
      "loss": 0.552,
      "step": 1060
    },
    {
      "epoch": 2.3056082362503387,
      "grad_norm": 0.3654361665248871,
      "learning_rate": 4.709302325581396e-05,
      "loss": 0.5602,
      "step": 1065
    },
    {
      "epoch": 2.3164454077485774,
      "grad_norm": 0.30585986375808716,
      "learning_rate": 4.636627906976744e-05,
      "loss": 0.5667,
      "step": 1070
    },
    {
      "epoch": 2.3272825792468166,
      "grad_norm": 0.3091560900211334,
      "learning_rate": 4.563953488372093e-05,
      "loss": 0.5361,
      "step": 1075
    },
    {
      "epoch": 2.3381197507450553,
      "grad_norm": 0.365744948387146,
      "learning_rate": 4.491279069767442e-05,
      "loss": 0.6702,
      "step": 1080
    },
    {
      "epoch": 2.3489569222432944,
      "grad_norm": 0.37247633934020996,
      "learning_rate": 4.418604651162791e-05,
      "loss": 0.5103,
      "step": 1085
    },
    {
      "epoch": 2.3597940937415336,
      "grad_norm": 0.3685409128665924,
      "learning_rate": 4.34593023255814e-05,
      "loss": 0.6254,
      "step": 1090
    },
    {
      "epoch": 2.3706312652397723,
      "grad_norm": 0.32254722714424133,
      "learning_rate": 4.2732558139534885e-05,
      "loss": 0.5539,
      "step": 1095
    },
    {
      "epoch": 2.3814684367380115,
      "grad_norm": 0.44013479351997375,
      "learning_rate": 4.200581395348838e-05,
      "loss": 0.6035,
      "step": 1100
    },
    {
      "epoch": 2.39230560823625,
      "grad_norm": 0.3100663721561432,
      "learning_rate": 4.127906976744187e-05,
      "loss": 0.5647,
      "step": 1105
    },
    {
      "epoch": 2.4031427797344893,
      "grad_norm": 0.3707042336463928,
      "learning_rate": 4.055232558139535e-05,
      "loss": 0.5903,
      "step": 1110
    },
    {
      "epoch": 2.4139799512327285,
      "grad_norm": 0.339883953332901,
      "learning_rate": 3.9825581395348835e-05,
      "loss": 0.5801,
      "step": 1115
    },
    {
      "epoch": 2.424817122730967,
      "grad_norm": 0.3460223972797394,
      "learning_rate": 3.909883720930232e-05,
      "loss": 0.5157,
      "step": 1120
    },
    {
      "epoch": 2.4356542942292063,
      "grad_norm": 0.332734078168869,
      "learning_rate": 3.837209302325582e-05,
      "loss": 0.5391,
      "step": 1125
    },
    {
      "epoch": 2.446491465727445,
      "grad_norm": 0.3490245044231415,
      "learning_rate": 3.7645348837209305e-05,
      "loss": 0.5618,
      "step": 1130
    },
    {
      "epoch": 2.457328637225684,
      "grad_norm": 0.2711634933948517,
      "learning_rate": 3.691860465116279e-05,
      "loss": 0.58,
      "step": 1135
    },
    {
      "epoch": 2.468165808723923,
      "grad_norm": 0.3676595985889435,
      "learning_rate": 3.619186046511628e-05,
      "loss": 0.6084,
      "step": 1140
    },
    {
      "epoch": 2.479002980222162,
      "grad_norm": 0.38880831003189087,
      "learning_rate": 3.5465116279069774e-05,
      "loss": 0.5693,
      "step": 1145
    },
    {
      "epoch": 2.4898401517204007,
      "grad_norm": 0.3227618336677551,
      "learning_rate": 3.4738372093023255e-05,
      "loss": 0.5774,
      "step": 1150
    },
    {
      "epoch": 2.50067732321864,
      "grad_norm": 0.33874866366386414,
      "learning_rate": 3.401162790697674e-05,
      "loss": 0.5843,
      "step": 1155
    },
    {
      "epoch": 2.511514494716879,
      "grad_norm": 0.38548702001571655,
      "learning_rate": 3.328488372093023e-05,
      "loss": 0.5726,
      "step": 1160
    },
    {
      "epoch": 2.5223516662151177,
      "grad_norm": 0.33778509497642517,
      "learning_rate": 3.2558139534883724e-05,
      "loss": 0.5432,
      "step": 1165
    },
    {
      "epoch": 2.533188837713357,
      "grad_norm": 0.32743164896965027,
      "learning_rate": 3.183139534883721e-05,
      "loss": 0.4927,
      "step": 1170
    },
    {
      "epoch": 2.5440260092115956,
      "grad_norm": 0.39220350980758667,
      "learning_rate": 3.11046511627907e-05,
      "loss": 0.5752,
      "step": 1175
    },
    {
      "epoch": 2.5548631807098348,
      "grad_norm": 0.35718169808387756,
      "learning_rate": 3.0377906976744187e-05,
      "loss": 0.5331,
      "step": 1180
    },
    {
      "epoch": 2.565700352208074,
      "grad_norm": 0.3504685163497925,
      "learning_rate": 2.9651162790697678e-05,
      "loss": 0.5785,
      "step": 1185
    },
    {
      "epoch": 2.5765375237063126,
      "grad_norm": 0.3255109190940857,
      "learning_rate": 2.8924418604651166e-05,
      "loss": 0.5749,
      "step": 1190
    },
    {
      "epoch": 2.5873746952045518,
      "grad_norm": 0.35911887884140015,
      "learning_rate": 2.8197674418604653e-05,
      "loss": 0.5853,
      "step": 1195
    },
    {
      "epoch": 2.5982118667027905,
      "grad_norm": 0.3835051655769348,
      "learning_rate": 2.747093023255814e-05,
      "loss": 0.608,
      "step": 1200
    },
    {
      "epoch": 2.6090490382010296,
      "grad_norm": 0.3486533761024475,
      "learning_rate": 2.674418604651163e-05,
      "loss": 0.5766,
      "step": 1205
    },
    {
      "epoch": 2.6198862096992683,
      "grad_norm": 0.36643868684768677,
      "learning_rate": 2.601744186046512e-05,
      "loss": 0.5481,
      "step": 1210
    },
    {
      "epoch": 2.6307233811975075,
      "grad_norm": 0.35014885663986206,
      "learning_rate": 2.5290697674418607e-05,
      "loss": 0.5537,
      "step": 1215
    },
    {
      "epoch": 2.641560552695746,
      "grad_norm": 0.32588836550712585,
      "learning_rate": 2.4563953488372094e-05,
      "loss": 0.5747,
      "step": 1220
    },
    {
      "epoch": 2.6523977241939853,
      "grad_norm": 0.3597162067890167,
      "learning_rate": 2.3837209302325582e-05,
      "loss": 0.5716,
      "step": 1225
    },
    {
      "epoch": 2.6632348956922245,
      "grad_norm": 0.3164665997028351,
      "learning_rate": 2.311046511627907e-05,
      "loss": 0.4807,
      "step": 1230
    },
    {
      "epoch": 2.674072067190463,
      "grad_norm": 0.3239942789077759,
      "learning_rate": 2.238372093023256e-05,
      "loss": 0.5257,
      "step": 1235
    },
    {
      "epoch": 2.6849092386887023,
      "grad_norm": 0.37953558564186096,
      "learning_rate": 2.1656976744186048e-05,
      "loss": 0.5717,
      "step": 1240
    },
    {
      "epoch": 2.695746410186941,
      "grad_norm": 0.33738455176353455,
      "learning_rate": 2.0930232558139536e-05,
      "loss": 0.5126,
      "step": 1245
    },
    {
      "epoch": 2.70658358168518,
      "grad_norm": 0.3531668186187744,
      "learning_rate": 2.0203488372093023e-05,
      "loss": 0.542,
      "step": 1250
    },
    {
      "epoch": 2.7174207531834194,
      "grad_norm": 0.3021634817123413,
      "learning_rate": 1.9476744186046514e-05,
      "loss": 0.4889,
      "step": 1255
    },
    {
      "epoch": 2.728257924681658,
      "grad_norm": 0.3620949983596802,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.5375,
      "step": 1260
    },
    {
      "epoch": 2.7390950961798968,
      "grad_norm": 0.3873251676559448,
      "learning_rate": 1.802325581395349e-05,
      "loss": 0.539,
      "step": 1265
    },
    {
      "epoch": 2.749932267678136,
      "grad_norm": 0.28814879059791565,
      "learning_rate": 1.7296511627906977e-05,
      "loss": 0.5123,
      "step": 1270
    },
    {
      "epoch": 2.760769439176375,
      "grad_norm": 0.36645588278770447,
      "learning_rate": 1.6569767441860464e-05,
      "loss": 0.4957,
      "step": 1275
    },
    {
      "epoch": 2.771606610674614,
      "grad_norm": 0.34469130635261536,
      "learning_rate": 1.5843023255813955e-05,
      "loss": 0.5906,
      "step": 1280
    },
    {
      "epoch": 2.782443782172853,
      "grad_norm": 0.38051629066467285,
      "learning_rate": 1.5116279069767441e-05,
      "loss": 0.547,
      "step": 1285
    },
    {
      "epoch": 2.7932809536710916,
      "grad_norm": 0.34420210123062134,
      "learning_rate": 1.4389534883720932e-05,
      "loss": 0.5583,
      "step": 1290
    },
    {
      "epoch": 2.804118125169331,
      "grad_norm": 0.35402676463127136,
      "learning_rate": 1.3662790697674418e-05,
      "loss": 0.5373,
      "step": 1295
    },
    {
      "epoch": 2.81495529666757,
      "grad_norm": 0.4212764501571655,
      "learning_rate": 1.2936046511627909e-05,
      "loss": 0.5758,
      "step": 1300
    },
    {
      "epoch": 2.8257924681658086,
      "grad_norm": 0.3405168354511261,
      "learning_rate": 1.2209302325581395e-05,
      "loss": 0.5715,
      "step": 1305
    },
    {
      "epoch": 2.836629639664048,
      "grad_norm": 0.31555482745170593,
      "learning_rate": 1.1482558139534884e-05,
      "loss": 0.5577,
      "step": 1310
    },
    {
      "epoch": 2.8474668111622865,
      "grad_norm": 0.33754387497901917,
      "learning_rate": 1.0755813953488372e-05,
      "loss": 0.5662,
      "step": 1315
    },
    {
      "epoch": 2.8583039826605257,
      "grad_norm": 0.34114548563957214,
      "learning_rate": 1.0029069767441861e-05,
      "loss": 0.5245,
      "step": 1320
    },
    {
      "epoch": 2.869141154158765,
      "grad_norm": 0.34155285358428955,
      "learning_rate": 9.302325581395349e-06,
      "loss": 0.5521,
      "step": 1325
    },
    {
      "epoch": 2.8799783256570035,
      "grad_norm": 0.372514545917511,
      "learning_rate": 8.575581395348838e-06,
      "loss": 0.5948,
      "step": 1330
    },
    {
      "epoch": 2.890815497155242,
      "grad_norm": 0.38241758942604065,
      "learning_rate": 7.848837209302325e-06,
      "loss": 0.5782,
      "step": 1335
    },
    {
      "epoch": 2.9016526686534814,
      "grad_norm": 0.3794563114643097,
      "learning_rate": 7.122093023255815e-06,
      "loss": 0.5624,
      "step": 1340
    },
    {
      "epoch": 2.9124898401517205,
      "grad_norm": 0.35047703981399536,
      "learning_rate": 6.395348837209303e-06,
      "loss": 0.5027,
      "step": 1345
    },
    {
      "epoch": 2.9233270116499592,
      "grad_norm": 0.32897046208381653,
      "learning_rate": 5.668604651162791e-06,
      "loss": 0.6023,
      "step": 1350
    }
  ],
  "logging_steps": 5,
  "max_steps": 1386,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.639117253541888e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
